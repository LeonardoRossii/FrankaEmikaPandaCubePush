GOAL:
You are a reinforcement learning expert trying to write reward function, success condition and failure condition to solve reinforcement learning task as effective as possibly.
Your should use useful variables from the environment that is provided to you.
Your goal is to write:
- A dense reward function for the environment that will help the agent learn the task described in the following lines.
- A success condition function for the environment to define if the task is successfully achieved.
- A failure condition function for the environment to define if the task is failed.
You must write a Python:
- Dense reward function get_reward(env,_) that returns (reward: float) importing all the necessary libraries.
- Function get_success_condition(env) that returns (success: bool)
- Function get_failure_condition(env) that returns (failure: bool)
Return only the executable Python function code without wrapping it in markdown backticks, namely not writing ```python and ```  respectively at the first and last lines. 

TASK:
The robot gripper is close to a cube, first it must reach the cube and and then with both the gripper fingers touch it and push it to the goal position.

SUCCESS CRITERIA:
The task is considered successfully completed when the target cube reaches the goal position, namely distance between them is lower than 0.04.

FAILURE CONDITION:
Consider the task failed if the distance of the end effector from the blue cube is more than 0.2m and if one Irreversible Event occurs.

IRREVERSIBLE EVENTS
Below is a list of Irreversible Events (IEs) associated with this task.
For each Irreversible Event:
Define, only if applicable, a dense penalty term that measures how far the agent currently is from triggering that event.
The definition must be consistent with both the environment class and the simplicity of its implementation.
If the environment includes safety filters or protective mechanisms that can prevent the occurrence of an Irreversible Event, you may use the filter’s effort or activation signal, as exposed in the environment API, to construct the dense penalty term.

Important:
If it is not possible to correctly, reliably, or meaningfully define a dense penalty term for a given Irreversible Event — even when considering available safety filter information — do not include that event in the reward shaping.
Finally, the sum of all valid dense penalty terms must be combined into a distinct reward-shaping term named irr_events_reward.

HINTS:
- Define dense shaping terms that clearly guides the agent for for task completion.
- After definig the other task shaping terms, define a terminal term of the reward function when the task is completed.
- The reward function should implement a shaping that clearly guides the policy towards the goal.
- The reward components should not be too disproportionate.
- Penalties for unwanted actions should be very small in absolute value, compared to positive rewards.

FORMALIZATION:
1. Terminal Reward Term:
- Think carefully and understand the possible maximum value for each positive reward term.
- Compute the sum of all the positive reward term's possible maximum values: max_rewards_bonuses
- Exactly define: reward += 10 * env.horizon * max_rewards_bonuses * int(env.check_success(env))
2. Final Reward
- The general reward function must be a convex combination between the Task associated terms and the Irreversible Event term
- reward = lambda*(reward) + (1-lambda)*irr_events_reward

STRUCTURE:
def get_reward(env,_,lambdas):
  rewards = []
  for lambda in lambdas:  
    reward = 0

    # Compute reward function shaping terms
    
    rewards.append(reward)
  
  return rewards

def get_success_condition(env):
    return success

def get_failure_condition(env):
    return failure